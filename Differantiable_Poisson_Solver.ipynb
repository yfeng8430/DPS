{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "# using float64 as default for each layer\n",
        "policy = tf.keras.mixed_precision.Policy(\"float64\")\n",
        "tf.keras.mixed_precision.set_global_policy(policy)"
      ],
      "metadata": {
        "id": "CUx_QIsoazZ1"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Gaussian Layer"
      ],
      "metadata": {
        "id": "PBukumb3f508"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# kernal function\n",
        "class Gaussian(layers.Layer):\n",
        "  def __init__(self, Nx=25, Ny=25, Lx=1.0, Ly=1.0, alpha=1/8, sigma=None):\n",
        "    super().__init__()\n",
        "\n",
        "    dx = Lx / Nx; dy = Ly / Ny;\n",
        "\n",
        "    if sigma==None:\n",
        "      sigma = alpha * (dx**2+dy**2)**0.5\n",
        "\n",
        "    x_c = tf.linspace(dx/2, Lx-dx/2, Nx)\n",
        "    y_c = tf.linspace(dy/2, Ly-dy/2, Ny)\n",
        "\n",
        "    x_mg, y_mg = tf.meshgrid(x_c, y_c)\n",
        "\n",
        "    self.x_mg = tf.reshape(tf.cast(x_mg, tf.float64),[1,1,-1])\n",
        "    self.y_mg = tf.reshape(tf.cast(y_mg, tf.float64),[1,1,-1])\n",
        "    self.sigma = sigma\n",
        "\n",
        "  def call(self, Source_XY, Source_Q):\n",
        "    Nb = tf.shape(Source_XY)[0]\n",
        "\n",
        "    x_src = Source_XY[:,:,0:1]\n",
        "    y_src = Source_XY[:,:,1:2]\n",
        "    q_src = Source_Q\n",
        "\n",
        "    D_square = tf.square(self.x_mg-x_src) + tf.square(self.y_mg-y_src)\n",
        "\n",
        "    gauss = tf.exp(-0.5 * D_square / self.sigma**2 )\n",
        "\n",
        "    b_star = gauss / tf.reduce_sum(gauss,axis=2,keepdims=True)\n",
        "    b_star *= q_src\n",
        "    b_star = tf.reduce_sum(b_star, axis=1)\n",
        "\n",
        "    return b_star"
      ],
      "metadata": {
        "id": "soM1bVEIV1TI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Poisson Layer"
      ],
      "metadata": {
        "id": "BsAsBFnygHUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Poisson(layers.Layer):\n",
        "  def __init__(self, Lx=1, Ly=1, Nx=25, Ny=25, Perm=None, viscosity=0.001, Ns=2):\n",
        "    super().__init__()\n",
        "\n",
        "    if Perm == None:\n",
        "      K_mu = tf.ones([Ny,Nx], tf.float64) * 200 * 1E-15 / viscosity\n",
        "    else:\n",
        "      K_mu = Perm / viscosity\n",
        "\n",
        "    self.Nx = Nx\n",
        "    self.Ny = Ny\n",
        "    self.N = Nx * Ny\n",
        "    self.dx = Lx / Nx\n",
        "    self.dy = Ly / Ny\n",
        "    self.Ns = Ns\n",
        "    self.Lx = Lx\n",
        "    self.Ly = Ly\n",
        "\n",
        "    paddings = tf.constant([[1, 1], [1, 1]])\n",
        "    K = tf.pad(K_mu, paddings, \"CONSTANT\")\n",
        "\n",
        "    self.Tim = (self.dy/self.dx)*2*K[1:-1, 1:-1]*K[1:-1, :-2]/(K[1:-1, 1:-1]+K[1:-1, :-2])\n",
        "    self.Tip = (self.dy/self.dx)*2*K[1:-1, 1:-1]*K[1:-1, 2:]/(K[1:-1, 1:-1]+K[1:-1, 2:])\n",
        "    self.Tjm = (self.dx/self.dy)*2*K[1:-1, 1:-1]*K[:-2, 1:-1]/(K[1:-1, 1:-1]+K[:-2, 1:-1])\n",
        "    self.Tjp = (self.dx/self.dy)*2*K[1:-1, 1:-1]*K[2:, 1:-1]/(K[1:-1, 1:-1]+K[2:, 1:-1])\n",
        "\n",
        "    self.A = tf.zeros([self.N, self.N], tf.float64)\n",
        "    self.A = tf.linalg.set_diag(self.A, tf.reshape(self.Tim, [-1])[1:], k=-1)\n",
        "    self.A = tf.linalg.set_diag(self.A, tf.reshape(self.Tip, [-1])[:-1], k=1)\n",
        "    self.A = tf.linalg.set_diag(self.A, tf.reshape(self.Tjp, [-1])[:self.N-Nx], k=Nx)\n",
        "    self.A = tf.linalg.set_diag(self.A, tf.reshape(self.Tjm, [-1])[Nx:], k=-Nx)\n",
        "\n",
        "    # Apply External Boundary Conditions - Constant Pressures at Left (1) and Right (0)\n",
        "    BCID_i = tf.concat([tf.zeros([Ny,1],tf.int32),tf.ones([Ny,1],tf.int32)*(Nx-1)], axis=0)\n",
        "    BCID_j = tf.reshape(tf.concat([tf.range(Ny),tf.range(Ny)], axis=0), [-1,1])\n",
        "    BCIDs  = tf.concat([BCID_j,BCID_i], axis=1)\n",
        "\n",
        "    T_bc_im = 2*(self.dy/self.dx)*K[1:-1, 1]\n",
        "    T_bc_ip = 2*(self.dy/self.dx)*K[1:-1,-2]\n",
        "    BCVal = tf.concat([T_bc_im, T_bc_ip], axis=0)\n",
        "\n",
        "    self.Tim = tf.tensor_scatter_nd_update(self.Tim, BCIDs[:Ny,:], BCVal[:Ny])\n",
        "    self.Tip = tf.tensor_scatter_nd_update(self.Tip, BCIDs[Ny:,:], BCVal[Ny:])\n",
        "    ###################################################################################\n",
        "\n",
        "    self.A = tf.linalg.set_diag(self.A, -tf.reshape(self.Tim+self.Tip+self.Tjp+self.Tjm, [-1]), k=0)\n",
        "\n",
        "    self.b = tf.zeros([self.N,1], tf.float64)\n",
        "\n",
        "    # Apply External Boundary Conditions - Constant Pressures at Left (1E+6) and Right (0)\n",
        "    self.Pbc_L = 1E+6\n",
        "    self.Pbc_R = 0\n",
        "    b_Val = -tf.concat([self.Tim[:,0]*self.Pbc_L, self.Tip[:,-1]*self.Pbc_R], axis=0)\n",
        "    b_IDs = BCIDs[:,0:1]*Nx+BCIDs[:,1:2]\n",
        "    self.b = tf.tensor_scatter_nd_add(self.b,\n",
        "                                      tf.concat([b_IDs, tf.zeros(b_IDs.shape,tf.int32)], axis=1),\n",
        "                                      b_Val)\n",
        "    ###################################################################################\n",
        "\n",
        "    self.A = tf.expand_dims(self.A, axis=0)\n",
        "    self.b = tf.expand_dims(self.b, axis=0)\n",
        "\n",
        "  def call(self, b_star):\n",
        "    Nb = tf.shape(b_star)[0]\n",
        "\n",
        "    # For Neumann BC - Constant Source Strength\n",
        "    A = tf.tile(self.A, [Nb,1,1])\n",
        "    b = tf.tile(self.b, [Nb,1,1])\n",
        "    b += tf.reshape(b_star, [Nb,-1,1] )\n",
        "    #############################################\n",
        "\n",
        "    pressure = tf.linalg.solve(A,b)\n",
        "\n",
        "    return pressure"
      ],
      "metadata": {
        "id": "hBl8ZPlCqFBe"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Source Localization using Poisson Layer"
      ],
      "metadata": {
        "id": "bjSHNQ9d1Mml"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Produce Target Pressure Field"
      ],
      "metadata": {
        "id": "hg6Bc5YM1IOx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "np.random.seed(10)\n",
        "\n",
        "Nx=25; Ny=25\n",
        "\n",
        "phi_ref = 0.2\n",
        "sigma = [0.01, 0.03, 0.05]\n",
        "\n",
        "phi_001 = np.random.normal(phi_ref, sigma[0], Nx*Ny)\n",
        "phi_003 = np.random.normal(phi_ref, sigma[1], Nx*Ny)\n",
        "phi_005 = np.random.normal(phi_ref, sigma[2], Nx*Ny)\n",
        "\n",
        "fig, axs = plt.subplots(2, 3, figsize=(6,4), sharey=True, tight_layout=True)\n",
        "\n",
        "axs[0,0].hist(phi_001, bins=200)\n",
        "axs[0,1].hist(phi_003, bins=200)\n",
        "axs[0,2].hist(phi_005, bins=200)\n",
        "axs[0,0].set_title('(a) $\\phi$ ($\\sigma=0.01$)', fontsize=12, y=-0.5)\n",
        "axs[0,0].set_xlim([0.15, 0.25])\n",
        "axs[0,0].set_ylim([0, 20])\n",
        "axs[0,1].set_title('(b) $\\phi$ ($\\sigma=0.03$)', fontsize=12, y=-0.5)\n",
        "axs[0,1].set_xlim([0.1, 0.3])\n",
        "axs[0,2].set_title('(c) $\\phi$ ($\\sigma=0.05$)', fontsize=12, y=-0.5)\n",
        "axs[0,2].set_xlim([0, 0.4])\n",
        "\n",
        "k_001 = 200*phi_001/phi_ref*(phi_001*(1-phi_ref)/phi_ref/(1-phi_001))**2\n",
        "k_003 = 200*phi_003/phi_ref*(phi_003*(1-phi_ref)/phi_ref/(1-phi_003))**2\n",
        "k_005 = 200*phi_005/phi_ref*(phi_005*(1-phi_ref)/phi_ref/(1-phi_005))**2\n",
        "\n",
        "axs[1,0].hist(k_001, bins=200)\n",
        "axs[1,1].hist(k_003, bins=200)\n",
        "axs[1,2].hist(k_005, bins=200)\n",
        "axs[1,0].set_title('(d) $k$ ($\\sigma=0.01$)', fontsize=12, y=-0.5)\n",
        "axs[1,0].set_xlim([100, 300])\n",
        "axs[1,1].set_title('(e) $k$ ($\\sigma=0.03$)', fontsize=12, y=-0.5)\n",
        "axs[1,1].set_xlim([0, 800])\n",
        "axs[1,1].set_xticks([0, 250, 500])\n",
        "axs[1,2].set_title('(f) $k$ ($\\sigma=0.05$)', fontsize=12, y=-0.5)\n",
        "axs[1,2].set_xlim([0, 1500])\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KchlyGKnkQy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Produce Target Pressure Field\n",
        "Nb=1; Ns=1; eps = 0.0001; Nx=25; Ny=25; Lx=1.0; Ly=1.0\n",
        "alpha = 1/8\n",
        "\n",
        "########## Generate a Permeability Field #########\n",
        "# Homogeneous Domain\n",
        "Perm_Homo = tf.ones([Ny,Nx], tf.float64) * 200 * 1E-15\n",
        "# Heterogeneous Domain\n",
        "Perm_Hete = tf.constant(k_005.reshape([Ny,Nx]), tf.float64) * 1E-15\n",
        "##################################################\n",
        "\n",
        "random_source = False\n",
        "if random_source:\n",
        "  # Randomly Generated Source Locations\n",
        "  x = tf.random.uniform(shape=[Nb,Ns,1], minval=eps, maxval=1-eps, dtype=tf.float64)\n",
        "  y = tf.random.uniform(shape=[Nb,Ns,1], minval=eps, maxval=1-eps, dtype=tf.float64)\n",
        "  SRC_XY = tf.concat([x,y], axis=2)\n",
        "  #####################################\n",
        "else:\n",
        "  # Customized Source Locations\n",
        "  one = tf.ones([Nb,1,1], tf.float64)\n",
        "  source_point_1 = tf.concat([0.05*one,0.05*one], axis=2)\n",
        "  source_point_2 = tf.concat([0.75*one,0.75*one], axis=2)\n",
        "  source_point_3 = tf.concat([0.25*one,0.75*one], axis=2)\n",
        "  source_point_4 = tf.concat([0.75*one,0.25*one], axis=2)\n",
        "  source_point_5 = tf.concat([0.5*one,0.5*one], axis=2)\n",
        "  SRC_XY = source_point_1\n",
        "  Ns = SRC_XY.shape[1]\n",
        "  ###############################################\n",
        "\n",
        "print (\"True Source Location:\", SRC_XY.numpy())\n",
        "\n",
        "SRC_Q = tf.ones([Nb,Ns,1], tf.float64) * (-1E-4)\n",
        "\n",
        "b_star = Gaussian(Nx=Nx, Ny=Ny, alpha=alpha)(SRC_XY, SRC_Q)\n",
        "P_target = Poisson(Nx=Nx, Ny=Ny, Perm=Perm_Homo)(b_star)\n",
        "\n",
        "# Image Plot\n",
        "fig, axs = plt.subplots(1, 2)\n",
        "axs[0].imshow(tf.reshape(b_star, [Nb,Ny,Nx])[0],cmap='gray',interpolation='nearest')\n",
        "axs[1].axis(\"off\")\n",
        "axs[1].imshow(tf.reshape(P_target,[Nb,Ny,Nx])[0],cmap='jet',interpolation='spline16')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fE8E6mSf1r1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Initial Guess - Source Locations"
      ],
      "metadata": {
        "id": "Ecr_HgGJgRpk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate Start Points for Source Localizations\n",
        "def generate_Initial_Sources(N_node_x, N_node_y):\n",
        "  x = tf.reshape(tf.linspace(Lx/N_node_x/2, 1-Lx/N_node_x/2, N_node_x), [-1,1])\n",
        "  y = tf.reshape(tf.linspace(Ly/N_node_y/2, 1-Ly/N_node_y/2, N_node_y), [-1,1])\n",
        "\n",
        "  x = tf.cast(x, tf.float64)\n",
        "  y = tf.cast(y, tf.float64)\n",
        "\n",
        "  x = tf.tile(x, [N_node_y,1])\n",
        "  y = tf.reshape(tf.tile(y, [1,N_node_x]), [N_node_x*N_node_y,1])\n",
        "\n",
        "  init_guess = tf.reshape(tf.concat([x,y], axis=1), [1,-1, 2])\n",
        "\n",
        "  return init_guess\n",
        "\n",
        "# set number of points to start localization\n",
        "num_inital_guess = 1\n",
        "\n",
        "one = tf.ones([1,1,1], tf.float64)\n",
        "if num_inital_guess == 1:\n",
        "  init_guess = tf.concat([0.5*one,0.5*one], axis=2)\n",
        "elif num_inital_guess == 5:\n",
        "  init_guess1 = tf.concat([0.25*one,0.25*one], axis=2)\n",
        "  init_guess2 = tf.concat([0.75*one,0.75*one], axis=2)\n",
        "  init_guess3 = tf.concat([0.25*one,0.75*one], axis=2)\n",
        "  init_guess4 = tf.concat([0.75*one,0.25*one], axis=2)\n",
        "  init_guess5 = tf.concat([0.5*one,0.5*one], axis=2)\n",
        "  init_guess = tf.concat([init_guess1,init_guess2,init_guess3,init_guess4,init_guess5], axis=1)\n",
        "else:\n",
        "  num_points = int(num_inital_guess**0.5+0.5)\n",
        "  init_guess = generate_Initial_Sources(num_points,num_points)\n",
        "\n",
        "print (init_guess)\n",
        "\n",
        "# Initialize src_q based on the number of initial guesses\n",
        "src_q = tf.ones([init_guess.shape[0],init_guess.shape[1],1], tf.float64) * (-1E-4)"
      ],
      "metadata": {
        "id": "bU1OWc0YMiWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Define Function - Mean Euclidean Distance"
      ],
      "metadata": {
        "id": "vHTutYYOVVNn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Euclidian Distance as the metrics\n",
        "def mean_euclidean_distance(true_xy, pred_xy):\n",
        "  pred_center_xy = tf.reduce_mean(pred_xy, axis=1)\n",
        "  true_center_xy = tf.reduce_mean(true_xy, axis=1)\n",
        "\n",
        "  med = tf.square(pred_center_xy-true_center_xy)\n",
        "  med = tf.reduce_sum(med, axis=1)\n",
        "\n",
        "  return tf.sqrt(med)"
      ],
      "metadata": {
        "id": "VMlAtNH7UBcq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Define Function - Plot Partial Pressure Field"
      ],
      "metadata": {
        "id": "mVhyr9TViXQq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_partial_pressure(p, sample, nx, ny, frac):\n",
        "  Psample = tf.reshape(sample[:int(frac*nx*ny)], [-1,1])\n",
        "  row = Psample[:int(frac*nx*ny)] // nx\n",
        "  col = Psample[:int(frac*nx*ny)] % nx\n",
        "  ids = tf.concat([row,col], axis=1)\n",
        "\n",
        "  P_partial = tf.gather(p[0,:,0], sample[:int(frac*nx*ny)])\n",
        "\n",
        "  scatter = tf.scatter_nd(ids, P_partial, [ny,nx])\n",
        "\n",
        "  vmin = tf.reduce_min(p[0,:,0])\n",
        "  vmax = tf.reduce_max(p[0,:,0])\n",
        "\n",
        "  fig, axs = plt.subplots(1, 2)\n",
        "  axs[0].axis(\"off\")\n",
        "  axs[0].imshow(tf.reshape(p,[1,Ny,Nx])[0], cmap='jet')\n",
        "  axs[1].axis(\"off\")\n",
        "  axs[1].imshow(tf.reshape(scatter,[1,Ny,Nx])[0], vmin=vmin, vmax=vmax, cmap='jet')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "f5LpPmUFug4q"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Source Localization\n",
        "Case I - Single Source\n",
        "1.   In Produce Target Pressure Module, set Ns=1\n",
        "2.   In Initial Guess Module, set num_inital_guess=1\n",
        "3.   In Source Localization Module, set data_frac = 1\n",
        "\n",
        "Case II - Multiple Sources\n",
        "1.   In Produce Target Pressure Module, set Ns=(larger than 1)\n",
        "2.   In Initial Guess Module, set num_inital_guess=(larger than 1)\n",
        "3.   In Source Localization Module, set data_frac = 1\n",
        "\n",
        "Case III - Partial Data\n",
        "1.   Define The Plot Function\n",
        "2.   In Source Localization Module, set data_frac = (between 0 and 1)"
      ],
      "metadata": {
        "id": "7QGum_YuABNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_src_list (src_ids, drop_ids, src_array, src_loc):\n",
        "  src_loc_flat = src_loc.reshape(-1)\n",
        "\n",
        "  new_row = np.empty((1,src_array.shape[1]))*np.nan\n",
        "\n",
        "  if (drop_ids.shape[0]>0):\n",
        "    src_ids = np.delete(src_ids, drop_ids)\n",
        "\n",
        "  src_ids_x = src_ids * 2\n",
        "  src_ids_y = src_ids * 2 + 1\n",
        "  src_ids_xy = np.concatenate((src_ids_x.reshape(-1,1), src_ids_y.reshape(-1,1)), axis=1)\n",
        "  src_ids_xy = src_ids_xy.reshape(-1)\n",
        "\n",
        "  new_row[0,src_ids_xy] = src_loc_flat\n",
        "\n",
        "  src_array = np.concatenate((src_array,new_row), axis=0)\n",
        "\n",
        "  return src_ids, src_array"
      ],
      "metadata": {
        "id": "aiJqxdr_vICB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alpha = 1/8\n",
        "gk = Gaussian(Nx=Nx, Ny=Ny, Lx=Lx, Ly=Ly, alpha=alpha)\n",
        "poisson = Poisson(Nx=Nx, Ny=Ny, Lx=Lx, Ly=Ly, Ns=Ns, Perm=Perm_Homo)\n",
        "\n",
        "epoch = 300\n",
        "\n",
        "all_src = []\n",
        "all_loss = []\n",
        "all_med = []\n",
        "all_iter = []\n",
        "best_src = []\n",
        "best_loss = []\n",
        "best_med = []\n",
        "best_iter = []\n",
        "all_src_np = np.array([]).reshape(0,init_guess.shape[1]*init_guess.shape[2])\n",
        "\n",
        "drop_ids = np.array([])\n",
        "src_ids = np.arange(init_guess.shape[1])\n",
        "\n",
        "src_loc = init_guess\n",
        "src_q = tf.ones([init_guess.shape[0],init_guess.shape[1],1], tf.float64) * (-1E-4)\n",
        "\n",
        "one = tf.ones([Nb,init_guess.shape[1],1], tf.float64)\n",
        "D_diag = (Lx**2+Ly**2)**0.5\n",
        "max_step_size = D_diag/100*one\n",
        "min_step_size = D_diag/1000*one\n",
        "\n",
        "samples = tf.range(0, Nx*Ny)\n",
        "samples = tf.random.shuffle(samples)\n",
        "data_frac = 1\n",
        "\n",
        "counter = 0\n",
        "d_src_loc = 0\n",
        "\n",
        "plot_partial_pressure(P_target, samples, Nx, Ny, data_frac)\n",
        "\n",
        "for i in range (epoch):\n",
        "  with tf.GradientTape(persistent=True, watch_accessed_variables=False) as tape:\n",
        "    tape.watch(src_loc)\n",
        "    source_field = gk(src_loc, src_q)\n",
        "    P_pred = poisson(source_field)\n",
        "\n",
        "    SE = tf.reshape(tf.square(P_pred-P_target),[-1])\n",
        "    loss = tf.reduce_mean(tf.gather(SE, samples[:int(data_frac*Nx*Ny)]))\n",
        "    med = mean_euclidean_distance(SRC_XY, src_loc)\n",
        "\n",
        "  if (i%1==0 and src_loc.shape[1]==1):\n",
        "    print (i, \"\\t\",\n",
        "           \"{:.5f}\".format(src_loc.numpy()[0,0,0]), \"\\t\",\n",
        "           \"{:.5f}\".format(src_loc.numpy()[0,0,1]), \"\\t\",\n",
        "           \"{:.5f}\".format(loss.numpy(),6), '\\t',\n",
        "           #\"{:.5f}\".format(loss_log10,6), '\\t',\n",
        "           \"{:.5f}\".format(med.numpy()[0]))\n",
        "  if (i%1==0 and src_loc.shape[1]>1):\n",
        "    print (i, src_loc.shape[1],\n",
        "           \"{:.5f}\".format(loss.numpy(),6), '\\t',\n",
        "           \"{:.5f}\".format(med.numpy()[0]))\n",
        "\n",
        "  ############################ Save Learning Data ############################\n",
        "  #if dropped_ids.shape[0] > 0:\n",
        "  src_ids, all_src_np = add_src_list(src_ids, drop_ids, all_src_np, src_loc.numpy())\n",
        "  all_loss.append(loss.numpy())\n",
        "  all_med.append(med.numpy()[0])\n",
        "  all_iter.append(i)\n",
        "  if(i==0):\n",
        "    best_src.append(all_src_np[i].tolist())\n",
        "    best_loss.append(loss.numpy())\n",
        "    best_med.append(med.numpy()[0])\n",
        "    best_iter.append(i)\n",
        "  else:\n",
        "    if loss.numpy() < min(best_loss):\n",
        "      best_src.append(all_src_np[i].tolist())\n",
        "      best_loss.append(loss.numpy())\n",
        "      best_med.append(med.numpy()[0])\n",
        "      best_iter.append(i)\n",
        "  #############################################################################\n",
        "\n",
        "  dx = tape.gradient(loss, src_loc)\n",
        "\n",
        "  ######################## Apply Gradients  #########################\n",
        "  d_move = loss / dx\n",
        "  D = tf.sqrt(tf.square(d_move[:,:,0:1])+tf.square(d_move[:,:,1:2]))\n",
        "  eta = tf.math.minimum(D,   max_step_size[:, :D.shape[1], :])\n",
        "  eta = tf.math.maximum(eta, min_step_size[:, :D.shape[1], :])\n",
        "  eps = tf.random.uniform(D.shape, minval=1, maxval=3, dtype=tf.float64)\n",
        "  eta = eta / D * eps\n",
        "  src_loc -= eta * d_move\n",
        "  ###################################################################\n",
        "\n",
        "  ########################### exclude false sources ###########################\n",
        "  in_range = tf.math.logical_and(src_loc > -0.01, src_loc < 1.01)\n",
        "  in_range = tf.math.logical_and(in_range[:,:,0], in_range[:,:,1])\n",
        "  drop_ids = np.array([])\n",
        "  if (in_range.shape[1] > 1):\n",
        "    drop_ids = tf.reshape(tf.where(tf.logical_not(tf.reshape(in_range,[-1]))),[-1]).numpy()\n",
        "    src_loc = tf.reshape(tf.gather_nd(src_loc, tf.where(in_range)), [1,-1,2])\n",
        "    src_q = tf.ones([src_loc.shape[0],src_loc.shape[1],1], tf.float64) * (-1E-4)\n",
        "  #############################################################################\n",
        "\n",
        "all_src = all_src_np.tolist()\n",
        "print (\"True Source Location:\", SRC_XY.numpy())\n",
        "print ('The best source coordiante is ', best_src[-1], 'with loss of ', best_loss[-1], 'MED of ', best_med[-1])"
      ],
      "metadata": {
        "id": "NctlB7eFgbwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Post-Processing"
      ],
      "metadata": {
        "id": "k8SBruF1M2wy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Define Function - Save to Excel"
      ],
      "metadata": {
        "id": "8F79jcY-kiSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def save_to_excel(filename, iter, loss, med, sources):\n",
        "    data = {'Iter': iter, 'Loss': loss, 'MED': med}\n",
        "    max_sources = max(len(source) for source in sources) // 2\n",
        "\n",
        "    for i in range(max_sources):\n",
        "        data[f'x{i+1}'] = []\n",
        "        data[f'y{i+1}'] = []\n",
        "\n",
        "    for index in range(len(iter)):\n",
        "        for i in range(max_sources):\n",
        "            if i < len(sources[index]) // 2:\n",
        "                data[f'x{i+1}'].append(sources[index][2*i])\n",
        "                data[f'y{i+1}'].append(sources[index][2*i+1])\n",
        "            else:\n",
        "                data[f'x{i+1}'].append(None)\n",
        "                data[f'y{i+1}'].append(None)\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_excel(filename, index=False)"
      ],
      "metadata": {
        "id": "0Tl230Lqki10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Save Predicted Path"
      ],
      "metadata": {
        "id": "vAecwiDbNJR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/My Drive/DPS/'\n",
        "filename = path + 'Case1.xlsx'\n",
        "save_to_excel(filename, best_iter, best_loss, best_med, best_src)"
      ],
      "metadata": {
        "id": "vrRih57Ykwbv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}